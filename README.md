# CS224N Custom Final Project

## Description

We build a sleeper agent on top of Llama-3-8B-Instruct. The sleeper agent exists at https://huggingface.co/ksw1/final-sleeper-agent-quant.

### Models
Each of our models use Direct Preference Optimization (DPO) to try to mitigate the misalignment within the sleeper agent. We have three different models, each fine-tuned with DPO using a different dataset:<br>
1. [DPO-1-10k](https://huggingface.co/ksw1/DPO-1-10k) and [DPO-1-1k](https://huggingface.co/ksw1/DPO-1-1k) <br>
2. [DPO-2-10k](https://huggingface.co/ksw1/DPO-1-10k) and [DPO-2-1k](https://huggingface.co/ksw1/DPO-1-1k) <br>
3. [DPO-3-10k](https://huggingface.co/ksw1/DPO-1-10k) and [DPO-3-1k](https://huggingface.co/ksw1/DPO-1-1k)

## Authors

[Katherine Worden](mailto:worden@stanford.edu) <br>
[Jeong Shin](mailto:jyshin@stanford.edu) <br>

Commits are joint.

## License


## Acknowledgments

* [README Template](https://gist.github.com/DomPizzie/7a5ff55ffa9081f2de27c315f5018afc)
